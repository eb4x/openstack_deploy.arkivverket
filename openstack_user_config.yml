---
cidr_networks: &cidr_networks
  container: 172.16.2.0/24
  storage: 172.16.3.0/24
  tunnel: 172.16.4.0/24

used_ips:
  - "172.16.2.0,172.16.2.63"    # static
# - "172.16.2.64,172.16.2.127"  # osa  /26
  - "172.16.2.128,172.16.2.191" # vrtx /26
# - "172.16.2.192,172.16.2.223" # osa  /27
  - "172.16.2.224,172.16.2.255" # vrtx /27

  - "172.16.3.0,172.16.3.63"    # static
# - "172.16.3.64,172.16.3.127"  # osa  /26
  - "172.16.3.128,172.16.3.191" # vrtx /26
# - "172.16.3.192,172.16.3.223" # osa  /27
  - "172.16.3.224,172.16.3.255" # vrtx /27

  - "172.16.4.0,172.16.4.63"    # static
# - "172.16.4.64,172.16.4.127"  # osa  /26
  - "172.16.4.128,172.16.4.191" # vrtx /26
# - "172.16.4.192,172.16.4.223" # osa  /27
  - "172.16.4.224,172.16.4.255" # vrtx /27

global_overrides:
  cidr_networks: *cidr_networks
  internal_lb_vip_address: 172.16.2.32 # osa
# internal_lb_vip_address: 172.16.2.33 # vrtx
  #
  # The below domain name must resolve to an IP address
  # in the CIDR specified in haproxy_keepalived_external_vip_cidr.
  # If using different protocols (https/http) for the public/internal
  # endpoints the two addresses must be different.
  #
  external_lb_vip_address: 10.20.21.32 # osa  openstack-test.arkivverket.no
# external_lb_vip_address: 10.20.21.33 # vrtx openstack.arkivverket.no

  tunnel_bridge: "br-vxlan"
  management_bridge: "br-mgmt"
  provider_networks:
    - network:
        container_bridge: "br-mgmt"
        container_type: "veth"
        container_interface: "eth1"
        ip_from_q: "container"
        type: "raw"
        group_binds:
          - all_containers
          - hosts
        is_container_address: true
    - network:
        container_bridge: "br-vxlan"
        container_type: "veth"
        container_interface: "eth10"
        ip_from_q: "tunnel"
        type: "vxlan"
        range: "1:1000"
        net_name: "vxlan"
        group_binds:
          - neutron_openvswitch_agent
    - network:
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth11"
        type: "vlan"
        range: "3:4,203:204,1201:1202"
        net_name: "vlan"
        group_binds:
          - neutron_openvswitch_agent
    - network:
        container_bridge: "br-storage"
        container_type: "veth"
        container_interface: "eth2"
        container_mtu: "9000"
        ip_from_q: "storage"
        type: "raw"
        group_binds:
          - glance_api
          - cinder_api
          - cinder_volume
          - nova_compute
          - ceph-osd

###
### Infrastructure
###

_infrastructure_hosts: &infrastructure_hosts
  proven-wombat:
    ip: 172.16.2.3
  guided-fawn:
    ip: 172.16.2.4
  grown-cobra:
    ip: 172.16.2.5

# nova hypervisors
compute_hosts: &compute_hosts
  giving-prawn:
    ip: 172.16.2.17
  apt-shiner:
    ip: 172.16.2.18

ceph-osd_hosts: &ceph-osd_hosts
  free-gnu:
    ip: 172.16.2.6
  ready-cat:
    ip: 172.16.2.7
  tough-rabbit:
    ip: 172.16.2.8
  new-martin:
    ip: 172.16.2.9

# galera, memcache, rabbitmq, utility
shared-infra_hosts: *infrastructure_hosts

# ceph-mon containers
ceph-mon_hosts: *infrastructure_hosts

# repository (apt cache, python packages, etc)
repo-infra_hosts: *infrastructure_hosts

# load balancer
# Ideally the load balancer should not use the Infrastructure hosts.
# Dedicated hardware is best for improved performance and security.
haproxy_hosts: *infrastructure_hosts

# rsyslog server
log_hosts:
  upward-teal:
    ip: 172.16.2.2

###
### OpenStack
###

# keystone
identity_hosts: *infrastructure_hosts

# cinder api services
storage-infra_hosts: *infrastructure_hosts

# cinder volume hosts (Ceph RBD-backed)
storage_hosts: *ceph-osd_hosts

# glance
image_hosts: *infrastructure_hosts

# nova api, conductor, etc services
compute-infra_hosts: *infrastructure_hosts

# heat
orchestration_hosts: *infrastructure_hosts

# horizon
dashboard_hosts: *infrastructure_hosts

# neutron server, agents (L3, etc)
network_hosts: *infrastructure_hosts

# ceilometer (telemetry data collection)
#metering-infra_hosts: *infrastructure_hosts

# aodh (telemetry alarm service)
#metering-alarm_hosts: *infrastructure_hosts

# gnocchi (telemetry metrics storage)
#metrics_hosts: *infrastructure_hosts

# ceilometer compute agent (telemetry data collection)
#metering-compute_hosts: *compute_hosts

# magnum
magnum-infra_hosts: *infrastructure_hosts
